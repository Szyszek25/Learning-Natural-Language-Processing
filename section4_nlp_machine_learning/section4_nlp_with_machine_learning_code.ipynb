{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379c8c23-56ce-4b24-8b6c-6e7297c41639",
   "metadata": {},
   "source": [
    "# NLP with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd474788-eedf-4441-bfc9-e42d194eb3c3",
   "metadata": {},
   "source": [
    "## 1. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c078b4-ece9-470d-bfb3-d24b2cc97723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When life gives you lemons, make lemonade! ðŸ™‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She bought 2 lemons for $1 at Maven Market.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A dozen lemons will make a gallon of lemonade. [AllRecipes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemon, lemon, lemons, lemon, lemon, lemons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He's running to the market to get a lemon â€” there's a great sale today.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  sentence\n",
       "0                             When life gives you lemons, make lemonade! ðŸ™‚\n",
       "1                              She bought 2 lemons for $1 at Maven Market.\n",
       "2              A dozen lemons will make a gallon of lemonade. [AllRecipes]\n",
       "3                               lemon, lemon, lemons, lemon, lemon, lemons\n",
       "4  He's running to the market to get a lemon â€” there's a great sale today."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#1. pandas\n",
    "\n",
    "# create a list of sentences\n",
    "data = [\n",
    "    \"When life gives you lemons, make lemonade! ðŸ™‚\",\n",
    "    \"She bought 2 lemons for $1 at Maven Market.\",\n",
    "    \"A dozen lemons will make a gallon of lemonade. [AllRecipes]\",\n",
    "    \"lemon, lemon, lemons, lemon, lemon, lemons\",\n",
    "    \"He's running to the market to get a lemon â€” there's a great sale today.\",\n",
    "    \"iced tea is my favorite\",\n",
    "    \"I didn't like the taste of that lemonade at all.\",\n",
    "    \"My lemons went bad before I could use them, unfortunately.\",\n",
    "] \n",
    "#2. ustawienie szerokosci kolumn\n",
    "# expand the column width to see the full sentences\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# turn it into a dataframe\n",
    "data_df = pd.DataFrame(data, columns=[\"sentence\"])\n",
    "data_df.head()\n",
    "#3. dataframe i nazwanie kolumny\n",
    "# make a copy of the dataframe\n",
    "df = data_df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1af4b9-1f04-444e-b6d8-4ba1cae0f776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When life gives you lemons, make lemonade! ðŸ™‚'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. import the VADER sentiment library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "text = df.sentence[0] #wybranie pierwsze elementu [0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45186878-1ac5-4589-91c0-15577cbe7585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.75, 'pos': 0.25, 'compound': 0.4587}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. create an analyzer object, apply it to the text data and view the polarity scores\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "analyzer.polarity_scores(text)\n",
    "# wybranie compound czyli mieszanka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95ea0bf-f919-4242-a849-7abe3e1ddd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When life gives you lemons, make lemonade! ðŸ™‚</td>\n",
       "      <td>0.4587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She bought 2 lemons for $1 at Maven Market.</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A dozen lemons will make a gallon of lemonade. [AllRecipes]</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemon, lemon, lemons, lemon, lemon, lemons</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He's running to the market to get a lemon â€” there's a great sale today.</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>iced tea is my favorite</td>\n",
       "      <td>0.4588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I didn't like the taste of that lemonade at all.</td>\n",
       "      <td>-0.2755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My lemons went bad before I could use them, unfortunately.</td>\n",
       "      <td>-0.7096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  sentence  \\\n",
       "0                             When life gives you lemons, make lemonade! ðŸ™‚   \n",
       "1                              She bought 2 lemons for $1 at Maven Market.   \n",
       "2              A dozen lemons will make a gallon of lemonade. [AllRecipes]   \n",
       "3                               lemon, lemon, lemons, lemon, lemon, lemons   \n",
       "4  He's running to the market to get a lemon â€” there's a great sale today.   \n",
       "5                                                  iced tea is my favorite   \n",
       "6                         I didn't like the taste of that lemonade at all.   \n",
       "7               My lemons went bad before I could use them, unfortunately.   \n",
       "\n",
       "   sentiment  \n",
       "0     0.4587  \n",
       "1     0.0000  \n",
       "2     0.0000  \n",
       "3     0.0000  \n",
       "4     0.6249  \n",
       "5     0.4588  \n",
       "6    -0.2755  \n",
       "7    -0.7096  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the sentiment analyzer to the entire dataframe\n",
    "\n",
    "# create an analyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# define a function to get the score\n",
    "def get_sentiment(text):\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "# apply the function\n",
    "df['sentiment'] = df['sentence'].apply(get_sentiment)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47919fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83ed30",
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_pl = [\n",
    "    \"LubiÄ™ placki ðŸ™‚\",\n",
    "    \"DostaÅ‚em 6 z matematyki! \",\n",
    "    \"Nie lubiÄ™ CiÄ™\",\n",
    "    \"Kocham szpital\",\n",
    "    \"Kocham szkoÅ‚Ä™\",\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f054e8",
   "metadata": {
    "vscode": {
     "languageId": "perl"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_pl_fr=pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6f415-74cf-43d6-abfa-3da102e877a3",
   "metadata": {},
   "source": [
    "## 2. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda936e-e9ac-459a-b835-fdfe68901d16",
   "metadata": {},
   "source": [
    "#### GOAL: Predict which reviews are high priority (vs low priority) that we need to address right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4b4657-b25d-402d-b7b8-16d5dc312734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065c1cfb-aac4-4db7-bf57-9c7eaf24ee8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23689</td>\n",
       "      <td>A21SYVGVNG8RAS</td>\n",
       "      <td>5</td>\n",
       "      <td>Low</td>\n",
       "      <td>Yummy snacks!</td>\n",
       "      <td>Popchips are the bomb!!  I use the parmesan garlic to scoop up cottage cheese as a healthy alternative to chips and dip.  My healthy eating program is saved.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23690</td>\n",
       "      <td>AQJYXC0MPRQJL</td>\n",
       "      <td>5</td>\n",
       "      <td>Low</td>\n",
       "      <td>Great chip that is different from the rest</td>\n",
       "      <td>I like the puffed nature of this chip that makes it more unique in the chip market.  I ordered the Salt and Vinegar and absolutely love that flavor, hands down my favorite chip ever.  I have tried the cheddar and regular flavors as well.  The cheddar is about a 4/5 and the regular is about a 3/5 because I prefer strong flavors and obviously that would not be the case for the regular.  The Salt and Vinegar is kind of weak compared to some regular S&amp;V chips, but is quite flavorful and makes you wanting to come back for more.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id          UserId  Rating Priority  \\\n",
       "0  23689  A21SYVGVNG8RAS       5      Low   \n",
       "1  23690   AQJYXC0MPRQJL       5      Low   \n",
       "\n",
       "                                        Title  \\\n",
       "0                               Yummy snacks!   \n",
       "1  Great chip that is different from the rest   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                     Popchips are the bomb!!  I use the parmesan garlic to scoop up cottage cheese as a healthy alternative to chips and dip.  My healthy eating program is saved.  \n",
       "1  I like the puffed nature of this chip that makes it more unique in the chip market.  I ordered the Salt and Vinegar and absolutely love that flavor, hands down my favorite chip ever.  I have tried the cheddar and regular flavors as well.  The cheddar is about a 4/5 and the regular is about a 3/5 because I prefer strong flavors and obviously that would not be the case for the regular.  The Salt and Vinegar is kind of weak compared to some regular S&V chips, but is quite flavorful and makes you wanting to come back for more.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the pop chip reviews\n",
    "reviews = pd.read_excel('../Data/Popchip_Reviews.xlsx')\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5df6ec5-11ad-4412-afb7-ea314651cb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 564 total reviews\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99c1ed41-811b-4de0-8276-6620d609f65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Priority\n",
       "Low     447\n",
       "High    117\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of low vs high priority reviews\n",
    "reviews.Priority.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "810d34fa-f579-46d8-80d0-4ad23f2da116",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'maven_text_preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# run this code in the command line if you get an error: python -m spacy download en_core_web_sm\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# import the text prepreocessing steps we created in the last section\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmaven_text_preprocessing\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# apply them to the reviews\u001b[39;00m\n\u001b[32m      7\u001b[39m reviews[\u001b[33m'\u001b[39m\u001b[33mText_Clean\u001b[39m\u001b[33m'\u001b[39m] = maven_text_preprocessing.clean_and_normalize(reviews[\u001b[33m'\u001b[39m\u001b[33mText\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'maven_text_preprocessing'"
     ]
    }
   ],
   "source": [
    "# run this code in the command line if you get an error: python -m spacy download en_core_web_sm\n",
    "\n",
    "# import the text prepreocessing steps we created in the last section\n",
    "import maven_text_preprocessing\n",
    "\n",
    "# apply them to the reviews\n",
    "reviews['Text_Clean'] = maven_text_preprocessing.clean_and_normalize(reviews['Text'])\n",
    "reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd733f4-433c-4eab-a174-8fa6bc9c84c2",
   "metadata": {},
   "source": [
    "#### ATTEMPT 1: Naive Bayes with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb07a38-e919-4d78-bd60-fe68565d56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer matrix\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=.2)\n",
    "X = cv.fit_transform(reviews.Text_Clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8bd488-4f8a-4be3-933f-f0ebf329c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the features / inputs X\n",
    "X_df = pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d60d9-313f-449f-bae1-f05ff687ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the target / output y\n",
    "y = reviews.Priority\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46069e-dbdd-4439-9eb7-b70b004831e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred_nb = model_nb.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9217b-fbfd-46a8-9d10-acc3a4c8be81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test it out on new reviews\n",
    "new_reviews = pd.Series([\n",
    "    \"Pop chips are my favorite! I love these chips so much.\",\n",
    "    \"Taste bad. I don't like the flavor options or taste.\",\n",
    "    \"Solid snack.\"\n",
    "])\n",
    "\n",
    "# clean and vectorize the new reviews using the same processes as earlier\n",
    "new_reviews_clean = maven_text_preprocessing.clean_and_normalize(new_reviews)\n",
    "new_reviews_df = pd.DataFrame(cv.transform(new_reviews_clean).toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "# make a prediction\n",
    "model_nb.predict(new_reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb53d86d-0247-4b62-9e0f-afd9da20f9d1",
   "metadata": {},
   "source": [
    "#### ATTEMPT 2: Logistic Regression with Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a90408-2c88-45c1-a09d-a90fbdcdfe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tfidf vectorizer matrix\n",
    "tv = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=.2)\n",
    "Xt = tv.fit_transform(reviews.Text_Clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91c7c5-6496-456a-8821-ca68094858fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the features / inputs X\n",
    "Xt_df = pd.DataFrame(Xt.toarray(), columns=tv.get_feature_names_out())\n",
    "Xt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfc179-479a-438c-86df-67550075ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the target / output y\n",
    "yt = reviews.Priority\n",
    "yt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965d283-33d3-48b3-b567-679f2c6f19d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train/test split\n",
    "Xt_train, Xt_test, yt_train, yt_test = train_test_split(Xt_df, yt, test_size=0.2, random_state=42)\n",
    "\n",
    "# model\n",
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(Xt_train, yt_train)\n",
    "\n",
    "# predict\n",
    "y_pred_lr = model_lr.predict(Xt_test)\n",
    "\n",
    "# evaluate\n",
    "print(classification_report(yt_test, y_pred_lr))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23445c-045f-4727-9316-cf2c587536ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest priority reviews\n",
    "import numpy as np\n",
    "\n",
    "reviews['predictions_nb'] = model_nb.predict_proba(X_df)[:, 0]\n",
    "reviews['predictions_lr'] = model_lr.predict_proba(Xt_df)[:, 0]\n",
    "reviews.sort_values('predictions_nb', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5146d2-7374-467d-9517-06fe36b31025",
   "metadata": {},
   "source": [
    "## 3. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c186b-3e00-410a-9309-1a45ae79bed8",
   "metadata": {},
   "source": [
    "#### GOAL: Find the main themes in the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10460778-e356-4c5e-b4f5-39e575a0613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new tfidf vectorizer with a lower document frequency range to capture more unique words\n",
    "tv2 = TfidfVectorizer(stop_words='english', min_df=0.05, max_df=.2)\n",
    "Xt2 = tv2.fit_transform(reviews.Text_Clean)\n",
    "Xt_df2 = pd.DataFrame(Xt2.toarray(), columns=tv2.get_feature_names_out())\n",
    "Xt_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a0e2a-30d4-4529-9af5-4a4e3113a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply nmf with n topics\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=5, random_state=42, max_iter=500)\n",
    "W = nmf.fit_transform(Xt_df2) # documents-topics\n",
    "H = nmf.components_ # topics-terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba588f1d-88be-47da-b0ad-271046c8852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 topics & 81 terms for each topic\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac91023-abee-4004-8cfb-b767f3e71a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view a single topic to term mapping\n",
    "H[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070b57e-eb9e-4259-8f51-a0461a7308d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display the top terms for each topic\n",
    "def display_topics(H, num_words=10):\n",
    "    for topic_num, topic_array in enumerate(H):\n",
    "        top_features = topic_array.argsort()[::-1][:num_words]\n",
    "        top_words = [tv2.get_feature_names_out()[i] for i in top_features]\n",
    "        print(\"Topic\", topic_num+1, \":\", ', '.join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9b28c-5532-4fee-9a49-e1145be4688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out the function\n",
    "display_topics(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c1d18-5f27-411c-aefb-b999ec1766ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents to topics\n",
    "doc_topics = pd.DataFrame(W)\n",
    "doc_topics.columns = ['orders', 'taste & texture', 'good', 'flavor', 'health']\n",
    "doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e0fee-ec8d-482d-a7cc-85183b744071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the reviews text with the topics\n",
    "reviews_topics = pd.concat([reviews.Text, doc_topics], axis=1)\n",
    "reviews_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f2156-214a-4a4f-bd72-8c51a2840c4a",
   "metadata": {},
   "source": [
    "### DEMO: Combine Multiple Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1413af3f-20b2-4ab6-b3a0-48fed07ef07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the final dataframe\n",
    "final_topics = reviews_topics.copy()\n",
    "final_topics.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b36497-2234-4cc6-b398-e73b06ba3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column that returns only the top topic\n",
    "final_topics['top_topic'] = final_topics.iloc[:, 1:].idxmax(axis=1)\n",
    "final_topics.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e0704-cb50-4f17-ae7b-65aebb2ec51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste down the sentiment function from earlier\n",
    "def get_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e065164-49e9-4daa-9bfb-9a67113e6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in our dataframe containing the sentiment scores\n",
    "final_topics['sentiment'] = final_topics.Text.apply(get_sentiment)\n",
    "final_topics.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea104ee2-3e84-47db-abb9-abb0ded604d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some eda on the data by finding average sentiment for each topic\n",
    "final_topics.groupby('top_topic')['sentiment'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34570ef-11d2-4da2-bbab-c1015b17cee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_machine_learning (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
